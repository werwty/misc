{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import syft as sy \n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self, epochs=1):\n",
    "        self.batch_size = 128\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = True\n",
    "        self.seed = 200226097 ## TODO change seed to your studentID inside the class Arguments (line 17)\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def train(args, model, device, federated_train_loader, optimizer, epoch, participates):\n",
    "    model.train()  # <-- initial training\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        if target.location.id in participates:\n",
    "            model.send(data.location) # <-- NEW: send the model to the right location\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            model.get() # <-- NEW: get the model back\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                loss = loss.get() # <-- NEW: get the loss back\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * args.batch_size, len(federated_train_loader) * args.batch_size,\n",
    "                    100. * batch_idx / len(federated_train_loader), loss.item()))\n",
    "\n",
    "\n",
    "            \n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "### main function\n",
    "def run(N=1, X=1):\n",
    "    args = Arguments(epochs = N)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(args.seed) \n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "    hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "\n",
    "    ## TODO: ----  create 10 node workers  ---- ##\n",
    "    worker_ids = []\n",
    "    for i in range(10):\n",
    "        worker_name = f\"worker{i}\"\n",
    "        worker = sy.VirtualWorker(hook, id=worker_name)\n",
    "        worker_ids.append(worker_name)\n",
    "\n",
    "    ##-------------------------------------------\n",
    "\n",
    "    ## distribute data across nodes\n",
    "    federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ]))\n",
    "        .federate((workers)), ##TODO: pass the worker nodes you created here to distribute the data\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    ## test dataset is always same at the central server\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    ## training models in a federated appraoch\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr) \n",
    "\n",
    "    ## TODO: select a random set of node ids that will be passed to the training function; these nodes will particiapte in the federated learning\n",
    "    #create node_list \n",
    "\n",
    "    node_list = random.sample(worker_ids, k=X)\n",
    "\n",
    "    ##-------------------------------------------\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, federated_train_loader, optimizer, epoch, node_list ) ## TODO: pass the node_id list like ['node1','node2' ...]\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "    if (args.save_model):\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 2a.\n",
      "Running with N=3, X=3\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 2.298272\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 2.150344\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 1.947209\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 1.381586\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.765479\n",
      "\n",
      "Test set: Average loss: 0.6228, Accuracy: 8326/10000 (83%)\n",
      "\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.588485\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.638507\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.453861\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.535925\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.244492\n",
      "\n",
      "Test set: Average loss: 0.3281, Accuracy: 9063/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.395385\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.261490\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.350926\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.268865\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.332211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2733, Accuracy: 9251/10000 (93%)\n",
      "\n",
      "Running with N=3, X=5\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 2.256382\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 2.065276\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 1.688400\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 1.179197\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.749203\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.580598\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.663559\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.504522\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.425321\n",
      "\n",
      "Test set: Average loss: 0.3872, Accuracy: 8912/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.364883\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.323480\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.278147\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.304774\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.469818\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.339882\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.289331\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.263013\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.422005\n",
      "\n",
      "Test set: Average loss: 0.2533, Accuracy: 9259/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.303857\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.329919\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.112009\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.227266\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.142510\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.206005\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.203432\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.246625\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.214898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1899, Accuracy: 9436/10000 (94%)\n",
      "\n",
      "Running with N=3, X=7\n",
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.306076\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.161293\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 1.686235\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 1.121563\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.573718\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.601819\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.578173\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.452109\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.382749\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.259029\n",
      "\n",
      "Test set: Average loss: 0.3239, Accuracy: 9020/10000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.352349\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.193373\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.233092\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.359445\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.207951\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.225892\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.308954\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.219897\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.378241\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.119934\n",
      "\n",
      "Test set: Average loss: 0.1917, Accuracy: 9428/10000 (94%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.244888\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.160364\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.092790\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.099470\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.217334\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.216244\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.137222\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.197587\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.197283\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.177594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1434, Accuracy: 9590/10000 (96%)\n",
      "\n",
      "Running with N=3, X=10\n",
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.306076\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.161293\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 1.911803\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 1.265940\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.804201\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.696759\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.362693\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.510118\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.441816\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.359350\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.459955\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.256071\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.427629\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.369672\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.300360\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.212879\n",
      "\n",
      "Test set: Average loss: 0.2595, Accuracy: 9223/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.292925\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.150638\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.197201\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.247813\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.179819\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.278692\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.151733\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.157159\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.229125\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.344769\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.252677\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.243731\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.195884\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.162515\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.292805\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.083210\n",
      "\n",
      "Test set: Average loss: 0.1446, Accuracy: 9579/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.183979\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.106433\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.194383\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.235732\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.072488\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.076912\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.161656\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.192525\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.117630\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.075211\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.106382\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.153076\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.127426\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.164563\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.169789\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.133713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1069, Accuracy: 9690/10000 (97%)\n",
      "\n",
      "Problem 2b.\n",
      "Running with X=3, N=3\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 2.189415\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 1.894555\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 1.259834\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.887583\n",
      "\n",
      "Test set: Average loss: 0.6186, Accuracy: 8551/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.590432\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.515363\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.482194\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.388739\n",
      "\n",
      "Test set: Average loss: 0.3951, Accuracy: 8729/10000 (87%)\n",
      "\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.184496\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mregister_response\u001b[0;34m(attr, response, response_ids, owner)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mambiguous_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mambiguous_methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9b63147c6d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mn_vals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Running with X=3, N={n}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c9a3031b776c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(N, X)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_list\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;31m## TODO: pass the node_id list like ['node1','node2' ...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-c9a3031b776c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, federated_train_loader, optimizer, epoch, participates)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparticipates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# <-- initial training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfederated_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# <-- now it is a distributed dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparticipates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- NEW: send the model to the right location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/frameworks/torch/fl/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/frameworks/torch/fl/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/frameworks/torch/fl/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# All the data for this worker has been used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/frameworks/torch/fl/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfederated_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# All the data for this worker has been used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/frameworks/torch/fl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTargets\u001b[0m \u001b[0mcorrespoding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mdatapoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdata_elem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;31m# TODO: avoid passing through numpy domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/pointers.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;31m# Send the command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, cmd_name, target, args_, kwargs_, return_ids, return_value)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mcmd_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m\"\"\"send message to worker location\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_pending_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# For backwards compatibility with Udacity course\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage_handlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;31m# TODO(karlhigley): Raise an exception if no handler is found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/abstract/message_handler.py\u001b[0m in \u001b[0;36mhandle\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_tensor_command\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute_tensor_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorCommandMessage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPointerTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComputationAction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_computation_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_communication_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/workers/message_handler.py\u001b[0m in \u001b[0;36mexecute_computation_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 response = hook_args.register_response(\n\u001b[0;32m--> 126\u001b[0;31m                     \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m                 )\n\u001b[1;32m    128\u001b[0m                 \u001b[0;31m# TODO: Does this mean I can set return_value to False and still\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mregister_response\u001b[0;34m(attr, response, response_ids, owner)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Update the function in cas of an error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0mregister_response_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_register_response_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;31m# Store this utility function in the registry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \u001b[0mregister_response_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregister_response_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_register_response_function\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;31m# Build a function with this rule to efficiently replace syft tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;31m# (but not pointer) with their child in the args_ objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m     \u001b[0mresponse_hook_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_register_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_hook_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/hw4-TP6T63vp-py3.7/lib/python3.7/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_register_response\u001b[0;34m(response, rules, return_tuple)\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;31m# Last if not, rule is probably == 1 so use type to return the right transformation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregister_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrules\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# And do this for all the responses / rules provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m     ]\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Problem 2a.\")\n",
    "x_vals = [3, 5, 7, 10]\n",
    "for x in x_vals:\n",
    "    print(f\"Running with N=3, X={x}\")\n",
    "    run(N=3, X=x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 2b.\n",
      "Running with X=5, N=3\n",
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.306076\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 2.161293\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 1.771049\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 1.143258\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.740875\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.672293\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.478192\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.561012\n",
      "\n",
      "Test set: Average loss: 0.3793, Accuracy: 8945/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.394081\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.244425\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.341500\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.308609\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.465418\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.369204\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.323698\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.292309\n",
      "\n",
      "Test set: Average loss: 0.2762, Accuracy: 9122/10000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.311858\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.214173\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.250592\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.247922\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.150934\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.183587\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.213984\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.191353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1860, Accuracy: 9437/10000 (94%)\n",
      "\n",
      "Running with X=5, N=5\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 2.256382\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 2.065276\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 1.773756\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 1.145211\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.740973\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.572766\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.657109\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.502181\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.425289\n",
      "\n",
      "Test set: Average loss: 0.3873, Accuracy: 8919/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.364866\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.324169\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.334088\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.311561\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.463665\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.334762\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.296561\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.263956\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.427957\n",
      "\n",
      "Test set: Average loss: 0.2550, Accuracy: 9238/10000 (92%)\n",
      "\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.307134\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.322183\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.248357\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.246696\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.147502\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.215139\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.206092\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.248725\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.218808\n",
      "\n",
      "Test set: Average loss: 0.1906, Accuracy: 9438/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.242547\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.318626\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.141539\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.153139\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.238811\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.226018\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.195264\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.181692\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.147762\n",
      "\n",
      "Test set: Average loss: 0.1699, Accuracy: 9489/10000 (95%)\n",
      "\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.096153\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.130131\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.089842\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.114064\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.133014\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.145667\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.152332\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.132839\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.158073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1281, Accuracy: 9601/10000 (96%)\n",
      "\n",
      "Running with X=5, N=10\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 2.256382\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 2.065276\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 1.958374\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 1.339540\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.877905\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.585238\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.655941\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.348908\n",
      "\n",
      "Test set: Average loss: 0.4004, Accuracy: 8749/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.365334\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.326093\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.464138\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.289958\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.303797\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.364029\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.305054\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.156041\n",
      "\n",
      "Test set: Average loss: 0.2449, Accuracy: 9279/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.310229\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.335869\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.140859\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.263828\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.243400\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.239208\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.216596\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.244085\n",
      "\n",
      "Test set: Average loss: 0.1967, Accuracy: 9440/10000 (94%)\n",
      "\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.244939\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.330907\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.179312\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.133983\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.139374\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.230142\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.205602\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.173136\n",
      "\n",
      "Test set: Average loss: 0.1539, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.098630\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.134626\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.121166\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.091431\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.092534\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.149688\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.154781\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.155303\n",
      "\n",
      "Test set: Average loss: 0.1377, Accuracy: 9581/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.114808\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.166840\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.109772\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.139884\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.123708\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.071164\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.162517\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.072094\n",
      "\n",
      "Test set: Average loss: 0.1088, Accuracy: 9672/10000 (97%)\n",
      "\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.205242\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.044376\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.091457\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.083840\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.221580\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.171240\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.151381\n",
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.065430\n",
      "\n",
      "Test set: Average loss: 0.0999, Accuracy: 9673/10000 (97%)\n",
      "\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.094851\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.138457\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.067048\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.145580\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.115021\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.133386\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.060195\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.115708\n",
      "\n",
      "Test set: Average loss: 0.0881, Accuracy: 9732/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.083169\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.072580\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.066407\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.069757\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.082408\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.194667\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.089467\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.067418\n",
      "\n",
      "Test set: Average loss: 0.0809, Accuracy: 9754/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.106134\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.090936\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.087079\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.115062\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.121275\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.090558\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.159469\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.119125\n",
      "\n",
      "Test set: Average loss: 0.0785, Accuracy: 9761/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Problem 2b.\")\n",
    "n_vals = [3, 5, 10]\n",
    "for n in n_vals:\n",
    "    print(f\"Running with X=5, N={n}\")\n",
    "    run(N=n, X=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
